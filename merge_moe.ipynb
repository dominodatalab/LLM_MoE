{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e14e3342-fcaa-4ea8-b015-a7a1d49eed3c",
   "metadata": {},
   "source": [
    "## Lets first create a MoE from the 2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb6fdf6-d043-494c-bc69-dcd8f456ebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Uncomment a d and use thoshis if tyou you have >=32 GB of RANM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84789c8c-4499-416b-b0d3-5968887cb1d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Now evaluate the merged model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a961b2cc-5252-4339-ae83-6ee3587b7d2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is the location of the merged model or the model path in Huggingface\n",
    "MODEL_ID = '/mnt/artifacts/merge/'\n",
    "# MODEL_ID = 'beowolx/CodeNinja-1.0-OpenChat-7B'\n",
    "# name of the model to append in the benchmark file\n",
    "model_name ='codeninja'\n",
    "# Name of the benchmark task to use for eval\n",
    "benchmark = 'eq_bench'\n",
    "# Flag to enable remote code\n",
    "TRUST_REMOTE_CODE = 'True'\n",
    "# Flag to load the model in 8 bit, 4 bit is also supported as a param in the command below\n",
    "load_in_8bit = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0262f600-052c-46fd-b40f-3a37906e7842",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Command to run the evaluation\n",
    "command = f\"lm_eval --model hf --model_args pretrained={MODEL_ID},dtype=auto,trust_remote_code={TRUST_REMOTE_CODE},load_in_8bit={load_in_8bit} --tasks {benchmark} --device cuda:0 --num_fewshot 0 --batch_size auto --output_path ./benchmark_{model_name}_{benchmark}.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0c50c9-31ba-47d5-aeaa-041a6dd55313",
   "metadata": {},
   "source": [
    "## Run the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0053260-1b9a-4011-8ebd-162f475c1de7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-05:19:07:08,178 INFO     [__main__.py:251] Verbosity set to INFO\n",
      "2024-04-05:19:07:13,219 INFO     [__main__.py:335] Selected Tasks: ['eq_bench']\n",
      "2024-04-05:19:07:13,219 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-04-05:19:07:13,219 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'beowolx/CodeNinja-1.0-OpenChat-7B', 'dtype': 'auto', 'trust_remote_code': True, 'load_in_8bit': True}\n",
      "2024-04-05:19:07:13,221 INFO     [huggingface.py:163] Using device 'cuda:0'\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:05<00:00,  1.71s/it]\n",
      "2024-04-05:19:07:18,826 WARNING  [big_modeling.py:422] You shouldn't move a model when it is dispatched on multiple devices.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2024-04-05:19:07:20,589 WARNING  [evaluator.py:239] Overwriting default num_fewshot of eq_bench from None to 0\n",
      "2024-04-05:19:07:20,590 INFO     [task.py:395] Building contexts for eq_bench on rank 0...\n",
      "100%|█████████████████████████████████████| 171/171 [00:00<00:00, 104536.65it/s]\n",
      "2024-04-05:19:07:20,597 INFO     [evaluator.py:379] Running generate_until requests\n",
      "Running generate_until requests:   0%|                  | 0/171 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\n",
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "Determined Largest batch size: 1\n",
      "Running generate_until requests: 100%|████████| 171/171 [17:11<00:00,  6.03s/it]\n",
      "hf (pretrained=beowolx/CodeNinja-1.0-OpenChat-7B,dtype=auto,trust_remote_code=True,load_in_8bit=True), gen_kwargs: (None), limit: None, num_fewshot: 0, batch_size: auto\n",
      "| Tasks  |Version|Filter|n-shot|     Metric      | Value  |   |Stderr|\n",
      "|--------|------:|------|-----:|-----------------|-------:|---|-----:|\n",
      "|eq_bench|    2.1|none  |     0|eqbench          | 64.7566|±  |2.6732|\n",
      "|        |       |none  |     0|percent_parseable|100.0000|±  |0.0000|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This will take around 21GB of VRAM on a single GPU for eq_bench for 2 7B merged MoE models\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fe82e1-5f47-42c6-984e-76285893a63e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
